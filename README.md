# <img src="./res/file/palbot_icon.png" width="80px" style="display:inline; vertical-align:middle"> PalBot

# Technological Innovations:
This project has developed a dual-arm wheeled desktop robot, with both the mechanical structure and system integration independently designed.

1. **Low-Cost, High-Degree-of-Freedom Robot Design**
![build_process](./res/file/build_process.png)
   - The robot's structure is entirely self-designed and manufactured via 3D printing, achieving extremely low production costs.
   - The robot features a dual-arm design, with each arm having 5 degrees of freedom (DoF) plus a gripper, and the head providing 2 DoF, totaling **14 DoF**.
   - Cost-effective motors are used for actuation, significantly reducing overall costs. The four-wheel drive employs a **Mecanum wheel** solution, with each motor costing around **$27**.

2. **Structural Optimization and Reachability Verification**
   - Through mechanical simulation and reachability analysis, the robot's dual arms are ensured to have a wide operational workspace, capable of desktop interaction and grasping tasks.

3. **Universal Intelligent Control Framework: Cradle**

![PalBot Framework](./res/file/PalBot%20Framework.png)

   - The self-developed **modular intelligent control framework "Cradle"** serves as the robot's brain, supporting six functional modules:
     - Information gathering
     - Self-reflection
     - Task reasoning
     - Skill management
     - Action planning
     - Memory
   - It enables **proactive conversation guidance, long-term memory**, and enhances human-robot emotional interaction.

4. **Physical Interaction Skill Library**
   - Capable of **continuously learning new skills** and maintaining an expandable skill library.
   - A robust skill library is built, including:
     - Grasping pose estimation
     - Visual tracking
     - Kinematic solving
     - Motion planning
   - This ensures **general and reliable grasping and physical interaction** capabilities.

# Demo Video:

[Skill Library Showcase](./res/file/video/Skill%20Library.mp4)

[Pick up Snack Showcase](./res/file/video/Pick%20up%20Snack.mp4)

[Pick up Trash Showcase](./res/file/video/Pick%20up%20Trash.mp4)

[Delivery a Birthday Card Showcase](./res/file/video/Delivery%20a%20Birthday%20Card.mp4)

[Dialog Showcase](./res/file/video/Dialog.mp4)

[Eyesight Tracking Showcase](./res/file/video/Eyesight%20Tracking.png)

# ðŸš€ Get Started
## Installation of Environment

```bash
conda create -n palbot-dev python=3.10.16
conda activate palbot-dev

pip install --upgrade torch==2.1.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
pip install torchvision==0.16.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

## Run the Project

Download the project and run the following command to start the project:

```bash
python -m runner
```

# ðŸŒ² Project Structure

```markdown
PalBot/
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ env_config_palbot.json         # environment config file (palbot only)
â”‚   â””â”€â”€ openai_config.json             # OpenAI API key config file
â”œâ”€â”€ hardware/
â”‚   â”œâ”€â”€ audio_manager.py                 # AudioManager class for managing microphone and speaker
â”‚   â”œâ”€â”€ camera.py                       # Camera class for realsense camera image capture
â”‚   â”œâ”€â”€ microphone.py                   # Microphone class for recording audio
â”‚   â”œâ”€â”€ multi_dynamixel_controller.py # MultiDynamixelController class for controlling multiple dynamixel motors
â”‚   â””â”€â”€ speaker.py                      # Speaker class for playing audio
â”œâ”€â”€ pal_agent/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.py                   # Configuration class for loading and managing configurations
â”‚   â”‚   â””â”€â”€ palbot_config.py            # Palbot configuration class for loading and managing palbot-specific configurations
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ palbot/
â”‚   â”‚   â”‚   â”œâ”€â”€ atomic_skills/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ interact.oy            # interact skill for palbot like move
â”‚   â”‚   â”‚   â”œâ”€â”€ composite_skills/
â”‚   â”‚   â”‚   â””â”€â”€ skill_registry.py          # SkillRegistry class for Palbot
â”‚   â”‚   â”œâ”€â”€ skill_registry_factory.py      # SkillRegistryFactory class for creating skill registries
â”‚   â”‚   â”œâ”€â”€ skill_registry.py              # SkillRegistry class for managing skills
â”‚   â”‚   â”œâ”€â”€ skill.py                      # Skill class
â”‚   â”‚   â””â”€â”€ utils.py                      # Utility functions for environment
â”‚   â”œâ”€â”€ gameio/
â”‚   â”‚   â””â”€â”€ game_manager.py               # GameManager class for managing state and actions
â”‚   â”œâ”€â”€ log/
â”‚   â”‚   â””â”€â”€ logger.py                     # Logger class for logging
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ base.py                       # Base class for memory
â”‚   â”‚   â””â”€â”€ local_memory.py               # LocalMemory class for managing local memory
â”‚   â”œâ”€â”€ module/
â”‚   â”‚   â””â”€â”€ executor.py                   # Executor class for executing actions
â”‚   â”œâ”€â”€ provider/
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”‚   â”œâ”€â”€ asr_provider.py           # ASRProvider class for speech recognition
â”‚   â”‚   â”‚   â””â”€â”€ tts_provider.py           # TTSProvider class for text-to-speech
â”‚   â”‚   â”œâ”€â”€ frame/
â”‚   â”‚   â”‚   â””â”€â”€ frame_provider.py          # FrameProvider class for managing frames
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_factory.py            # LLMFactory class for creating LLM providers
â”‚   â”‚   â”‚   â””â”€â”€ openai_provider.py         # OpenAIProvider class for OpenAI API
â”‚   â”‚   â”œâ”€â”€ palbot/
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py                 # Client class for palbot
â”‚   â”‚   â”‚   â””â”€â”€ palbot_interface.py       # PalbotInterface class for palbot
â”‚   â”‚   â”œâ”€â”€ video/
â”‚   â”‚   â”‚   â”œâ”€â”€ video_ocr_provider.py     # VideoOCRProvider class for video OCR
â”‚   â”‚   â”‚   â””â”€â”€ video_provider.py         # VideoProvider class for video
â”‚   â”‚   â””â”€â”€ base_provider.py              # BaseProvider class for providers
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ audio_utils.py                # Audio utility functions
â”‚   â”‚   â”œâ”€â”€ check.py                      # Check utility functions
â”‚   â”‚   â”œâ”€â”€ dict_utils.py                 # Dictionary utility functions
â”‚   â”‚   â”œâ”€â”€ encoding_utils.py             # Encoding utility functions
â”‚   â”‚   â”œâ”€â”€ file_utils.py                 # File utility functions
â”‚   â”‚   â”œâ”€â”€ image_utils.py                # Image utility functions
â”‚   â”‚   â”œâ”€â”€ json_utils.py                 # JSON utility functions
â”‚   â”‚   â”œâ”€â”€ singleton.py                  # Singleton utility functions
â”‚   â”‚   â””â”€â”€ string_utils.py               # String utility functions
â”‚   â””â”€â”€ constants.py                      # Constants for the project
â”œâ”€â”€ res/
â”‚   â”œâ”€â”€ file/                             # Resource files
â”‚   â”œâ”€â”€ prompts/                          # Prompt files
â”‚   â””â”€â”€ skills/                           # Skill files
â”œâ”€â”€ runs/                                 # Log files
â”œâ”€â”€ .env                                  # Environment variables
â”œâ”€â”€ .gitignore                            # Git ignore file
â”œâ”€â”€ README.md                             # Project README
â”œâ”€â”€ requirement.txt                       # Project requirements
â””â”€â”€ runner.py                             # Main runner file
```

# Acknowledgements

This project is based on the following projects:

**Cradle:** [Website](https://baai-agents.github.io/Cradle/), [GitHub](https://github.com/BAAI-Agents/Cradle)

**QuadWBG:** [Website](https://quadwbg.github.io/), [GitHub](https://github.com/javokhirajabov/quadwbg)

**Being-0:** [Website](https://beingbeyond.github.io/being-0/), [GitHub](https://github.com/BeingBeyond/being-0)

# Citation
If you find our work useful, please consider citing us and star our repository!

```
@article{tan2024cradle,
  title={Cradle: Empowering Foundation Agents towards General Computer Control},
  author={Weihao Tan and Wentao Zhang and Xinrun Xu and Haochong Xia and Ziluo Ding and Boyu Li and Bohan Zhou and Junpeng Yue and Jiechuan Jiang and Yewen Li and Ruyi An and Molei Qin and Chuqiao Zong and Longtao Zheng and Yujie Wu and Xiaoqiang Chai and Yifei Bi and Tianbao Xie and Pengjie Gu and Xiyun Li and Ceyao Zhang and Long Tian and Chaojie Wang and Xinrun Wang and BÃ¶rje F. Karlsson and Bo An and Shuicheng Yan and Zongqing Lu},
  journal={arXiv preprint arXiv:2403.03186},
  year={2024}
}

@article{wang2024quadwbg,
  title={QuadWBG: Generalizable Quadrupedal Whole-Body Grasping},
  author={Wang, Jilong and Rajabov, Javokhirbek and Xu, Chaoyi and Zheng, Yiming and Wang, He},
  journal={arXiv preprint arXiv:2411.06782},
  year={2024}
}

@article{yuan2025being,
    title={Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills},
    author={Yuan, Haoqi and Bai, Yu and Fu, Yuhui and Zhou, Bohan and Feng, Yicheng and Xu, Xinrun and Zhan, Yi and Karlsson, B{\"o}rje F and Lu, Zongqing},
    journal={arXiv preprint arXiv:2503.12533},
    year={2025}
}
```
